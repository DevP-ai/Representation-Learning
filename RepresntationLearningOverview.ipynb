{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4bdaec",
   "metadata": {},
   "source": [
    "# Representation Learning\n",
    "\n",
    "1.\tRepresentation learning is a very important aspect of machine learning which automatically discovers the feature patterns in the data.\n",
    "2.\tWhen the machine is provided with the data, it learns the representation itself without any human intervention.\n",
    "3.\tThe goal of representation learning is to train machine learning algorithms to learn useful representations, such as those that are interpretable, incorporate latent features, or can be used for transfer learning.\n",
    "\n",
    "Need of Representation Learning\n",
    "1.\tAssume we ’re developing a machine-learning algorithm to predict dog breeds based on pictures.\n",
    "2.\tBecause image data provides all of the answers, we must rely heavily on it when developing the algorithm.\n",
    "3.\tEach observation or feature in the data describes the qualities of the dogs.\n",
    "4.\tThe machine learning system that predicts the outcome must comprehend how each attribute interacts with other outcomes such as Pug, Golden Retriever, and so on.\n",
    "5.\tAs a result, if there is any noise or irregularity in the input, the result can be drastically different, which is a risk with most machine learning algorithms.\n",
    "6.\tThe majority of machine learning algorithms have only a basic understanding of the data.\n",
    "7.\tSo in such cases, the solution is to provide a more abstract representation of data.\n",
    "8.\tIt’s impossible to tell which features should be extracted for many tasks. This is where the concept of representation learning takes shape.\n",
    "What is Representation Learning?\n",
    "1.\tRepresentation learning is a class of machine learning approaches that allow a system to discover the representations required for feature detection or classification from raw data.\n",
    "2.\tThe requirement for manual feature engineering is reduced by allowing a machine to learn the features and apply them to a given activity.\n",
    "3.\tIn representation learning, data is sent into the machine, and it learns the representation on its own.\n",
    "4.\tIt is a way of determining a data representation of the features, the distance function, and the similarity function that determines how the predictive model will perform.\n",
    "5.\tRepresentation learning works by reducing high-dimensional data to low-dimensional data, making it easier to discover patterns and anomalies while also providing a better understanding of the data’s overall behaviour.\n",
    "\n",
    "\n",
    "\n",
    "6.\tBasically, Machine learning tasks such as classification frequently demand input that is mathematically and computationally convenient to process, which motivates representation learning.\n",
    "7.\tReal-world data, such as photos, video, and sensor data, has resisted attempts to define certain qualities algorithmically.\n",
    "8.\tAn approach is to examine the data for such traits or representations rather than depending on explicit techniques.\n",
    "\n",
    "\n",
    "Methods of Representation Learning\n",
    "\n",
    "We must employ representation learning to ensure that the model provides invariant and untangled outcomes in order to increase its accuracy and performance.\n",
    "1.\tSupervised Learning\n",
    "i.\tThis is referred to as supervised learning when the Machine Learning  or Deep Learning  model maps the input X to the output Y.\n",
    "ii.\tThe computer tries to correct itself by comparing model output to ground truth, and the learning process optimizes the mapping from input to output.\n",
    "iii.\tThis process is repeated until the optimization function reaches global minima.\n",
    "iv.\tEven when the optimization function reaches the global minima, new data does not always perform well, resulting in overfitting.\n",
    "v.\tWhile supervised learning does not necessitate a significant amount of data to learn the mapping from input to output, it does necessitate the learned features.\n",
    "vi.\tThe prediction accuracy can improve by up to 17 percent when the learned attributes are incorporated into the supervised learning algorithm.\n",
    "Using labelled input data, features are learned in supervised feature learning. Supervised neural networks, multilayer perceptions, and (supervised) dictionary learning are some examples.\n",
    "2.\tUnsupervised Learning\n",
    "i.\tUnsupervised learning is a sort of machine learning in which the labels are ignored in favour of the observation itself.\n",
    "ii.\tUnsupervised learning isn’t used for classification or regression; instead, it’s used to uncover underlying patterns, cluster data, denoise it, detect outliers, and decompose data, among other things.\n",
    "iii.\tWhen working with data x, we must be very careful about whatever features z we use to ensure that the patterns produced are accurate.\n",
    "iv.\tIt has been observed that having more data does not always imply having better representations.\n",
    "v.\tWe must be careful to develop a model that is both flexible and expressive so that the extracted features can convey critical information.\n",
    "Unsupervised feature learning learns features from unlabelled input data by following the methods such as\n",
    "1.\tDictionary learning\n",
    "2.\tIndependent component analysis\n",
    "3.\tAutoencoders\n",
    "4.\tMatrix factorization\n",
    "and various forms of clustering are among examples.\n",
    "\n",
    "\n",
    "Supervised Methods\n",
    "Supervised Dictionary Learning\n",
    "1.\tDictionary learning creates a set of representative elements (dictionary) from the input data, allowing each data point to be represented as a weighted sum of the representative elements.\n",
    "2.\tBy minimizing the average representation error (across the input data) and applying L1 regularization to the weights, the dictionary items and weights may be obtained\n",
    "i.e., the representation of each data point has only a few       nonzero weights.\n",
    "3.\tFor optimizing dictionary elements, supervised dictionary learning takes advantage of both the structure underlying the input data and the labels.\n",
    "\n",
    "4.\tThe supervised dictionary learning technique uses dictionary learning to solve classification issues by optimizing dictionary elements, data point weights, and classifier parameters based on the input data.\n",
    "\n",
    "A minimization problem is formulated, with the objective function consisting of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse data representation), and an L2 regularization on the parameters of the classification algorithm.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Multi-Layer Perceptron\n",
    "1.\tThe perceptron is the most basic neural unit, consisting of a succession of inputs and weights that are compared to the ground truth.\n",
    "2.\tA multi-layer perceptron, or MLP, is a feed-forward neural network made up of layers of perceptron units.\n",
    "3.\tMLP is made up of three-node layers: an input, a hidden layer, and an output layer.\n",
    "4.\tMLP is made up of three-node layers: an input, a hidden layer, and an output layer.\n",
    " \n",
    "\n",
    "5.\tThis notion serves as a foundation for hidden variables and representation learning.\n",
    "6.\tOur goal in this theorem is to determine the variables or required weights that can represent the underlying distribution of the entire data so that when we plug those variables or required weights into unknown data, we receive results that are almost identical to the original data.\n",
    "7.\tIn a word, artificial neural networks (ANN) assist us in extracting meaningful patterns from a dataset.\n",
    "Neural Networks\n",
    "1.\tNeural networks are a class of learning algorithms that employ a “network” of interconnected nodes in various layers.\n",
    "2.\tIt’s based on the animal nervous system, with nodes resembling neurons and edges resembling synapses.\n",
    "3.\tThe network establishes computational rules for passing input data from the network’s input layer to the network’s output layer, and each edge has an associated weight.\n",
    "4.\tThe relationship between the input and output layers, which is parameterized by the weights, is described by a network function associated with a neural network.\n",
    "5.\tVarious learning tasks can be achieved by minimizing a cost function over the network function (w) with correctly defined network functions.\n",
    "\n",
    "Unsupervised Method\n",
    "1.\tLearning Representation from unlabelled data is referred to as unsupervised feature learning.\n",
    "2.\tUnsupervised Representation learning frequently seeks to uncover low-dimensional features that encapsulate some structure beneath the high-dimensional input data.\n",
    "\n",
    "K-Mean Clustering\n",
    "1.\tK-means clustering is a vector quantization approach.\n",
    "2.\tAn n-vector set is divided into k clusters (i.e. subsets) via K-means clustering, with each vector belonging to the cluster with the closest mean.\n",
    "3.\tDespite the use of inferior greedy techniques, the problem is computationally NP-hard.\n",
    "4.\tK-means clustering divides an unlabelled collection of inputs into k groups before obtaining centroids-based features.\n",
    "5.\tThese characteristics can be honed in a variety of ways.\n",
    "6.\tThe simplest method is to add k binary features to each sample, with each feature j having a value of one of the k-means learned jth centroid is closest to the sample under consideration.\n",
    "7.\tCluster distances can be used as features after being processed with a radial basis function.\n",
    "\n",
    "Local Linear Embedding(LLE)\n",
    "1.\tLLE is a nonlinear learning strategy for constructing low-dimensional neighbour-preserving representations from high-dimensional (unlabelled) input.\n",
    "2.\tLLE’s main goal is to reconstruct high-dimensional data using lower-dimensional points while keeping some geometric elements of the original data set’s neighbours.\n",
    "3.\tThere are two major steps in LLE. The first step is “neighbour-preserving,” in which each input data point Xi is reconstructed as a weighted sum of K nearest neighbour data points, with the optimal weights determined by minimizing the average squared reconstruction error (i.e., the difference between an input point and its reconstruction) while keeping the weights associated with each point equal to one.\n",
    "\n",
    "4.\tThe second stage involves “dimension reduction,” which entails searching for vectors in a lower-dimensional space that reduce the representation error while still using the optimal weights from the previous step.\n",
    "\n",
    "5.\tThe weights are optimized given fixed data in the first stage, which can be solved as a least-squares problem.\n",
    "\n",
    "6.\tLower-dimensional points are optimized with fixed weights in the second phase, which can be solved using sparse eigenvalue decomposition.\n",
    "\n",
    "\n",
    "Unsupervised Dictionary Mining\n",
    "1.\tFor optimizing dictionary elements, unsupervised dictionary learning does not use data labels and instead relies on the structure underlying the data.\n",
    "2.\tSparse coding, which seeks to learn basic functions (dictionary elements) for data representation from unlabelled input data, is an example of unsupervised dictionary learning.\n",
    "3.\tWhen the number of vocabulary items exceeds the dimension of the input data, sparse coding can be used to learn overcomplete dictionaries.\n",
    "4.\tK-SVD is an algorithm for learning a dictionary of elements that allows for sparse representation.\n",
    "\n",
    "Deep Architectures Methods\n",
    "1.\tDeep learning architectures for feature learning are inspired by the hierarchical architecture of the biological brain system, which stacks numerous layers of learning nodes.\n",
    "2.\tThe premise of distributed representation is typically used to construct these architectures: observable data is generated by the interactions of many diverse components at several levels.\n",
    "\n",
    "Restricted Boltzmann Machine (RBMs)\n",
    "1.\tIn multilayer learning frameworks, RBMs (restricted Boltzmann machines) are widely used as building blocks.\n",
    "2.\tAn RBM is a bipartite undirected network having a set of binary hidden variables, visible variables, and edges connecting the hidden and visible nodes.\n",
    "3.\tIt’s a variant of the more general Boltzmann machines, with the added constraint of no intra-node connections.\n",
    "4.\tIn an RBM, each edge has a weight assigned to it.\n",
    "5.\tThe connections and weights define an energy function that can be used to generate a combined distribution of visible and hidden nodes.\n",
    "6.\tFor unsupervised representation learning, an RBM can be thought of as a single-layer design.\n",
    "7.\tThe visible variables, in particular, relate to the input data, whereas the hidden variables correspond to the feature detectors.\n",
    "8.\tHinton’s contrastive divergence (CD) approach can be used to train the weights by maximizing the probability of visible variables.\n",
    "Autoencoders\n",
    "\n",
    "1.\tDeep network representations have been found to be insensitive to complex noise or data conflicts.\n",
    "2.\tThis can be linked to the architecture to some extent. \n",
    "3.\tThe employment of convolutional layers and max-pooling, for example, can be proven to produce transformation insensitivity. \n",
    "\n",
    " \n",
    "\n",
    "4.\tAutoencoders are therefore neural networks that may be taught to do representation learning. \n",
    "5.\t Autoencoders seek to duplicate their input to their output using an encoder and a decoder.\n",
    "6.\tAutoencoders are typically trained via recirculation, a learning process that compares the activation of the input network to the activation of the reconstructed input.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d301bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
